from fastapi import FastAPI, HTTPException, Query, Response
from pydantic import BaseModel
from typing import Dict, Any, Optional, List, Union
import os, httpx, asyncio, datetime, socket, time
from consistent_hash import ConsistentHashRing
from ddtrace import patch_all, tracer
from ddtrace.vendor.dogstatsd import DogStatsd
import logging
import json

patch_all()

# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è DogStatsD –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫
statlogger = DogStatsd(host=os.getenv("DD_AGENT_HOST", "datadog"), port=8125)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —à–∞—Ä–¥–æ–≤
logging.basicConfig(level=logging.INFO)
shard_logger = logging.getLogger("shard")


def shard_structured_log(operation: str, key: str = None, status: str = "success",
                         error: str = None, duration_ms: float = None, **kwargs):
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —à–∞—Ä–¥–æ–≤"""
    span = tracer.current_span()
    log_data = {
        "timestamp": datetime.datetime.utcnow().isoformat(),
        "level": "ERROR" if error else "INFO",
        "service": "shard-service",
        "shard": SHARD_NAME,
        "role": REPLICA_ROLE,
        "group": SHARD_GROUP,
        "operation": operation,
        "trace_id": span.trace_id if span else None,
        "key": key,
        "status": status,
        "duration_ms": duration_ms,
        **kwargs
    }

    if error:
        log_data["error"] = error
        shard_logger.error(json.dumps(log_data))
    else:
        shard_logger.info(json.dumps(log_data))


app = FastAPI(title="Shard Node with Replication Log and Monitoring")

# --- –ö–æ–Ω—Ñ—ñ–≥ ---
COORDINATOR_URL = os.getenv("COORDINATOR_URL")
SHARD_NAME = os.getenv("SHARD_NAME")
REPLICA_ROLE = os.getenv("REPLICA_ROLE", "leader")
SHARD_GROUP = os.getenv("SHARD_GROUP")
FOLLOWER_URLS = os.getenv("FOLLOWER_URLS", "").split(",") if os.getenv("FOLLOWER_URLS") else []
LEADER_URL = os.getenv("LEADER_URL")

NODE_URL = f"http://{SHARD_NAME}:8000"

# --- STORAGE + LOG ---
STORAGE: Dict[str, Dict[str, Any]] = {}
EVENT_LOG: List[Dict[str, Any]] = []

# –°—á–µ—Ç—á–∏–∫–∏ –¥–ª—è –º–µ—Ç—Ä–∏–∫
SHARD_COUNTERS = {
    "write_success": 0,
    "write_error": 0,
    "read_success": 0,
    "read_error": 0,
    "replication_received": 0,
    "replication_errors": 0,
    "sync_requests": 0
}


class KeyValue(BaseModel):
    table: str
    key: str
    sort_key: Optional[str] = None
    value: Any  # <--- –í–ê–ñ–õ–ò–í–û: Any, —â–æ–± –ø—Ä–∏–π–º–∞—Ç–∏ —ñ —Å–ø–∏—Å–∫–∏, —ñ —Å–ª–æ–≤–Ω–∏–∫–∏


# --- Vector Clock –¥–ª—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç-—Ä–µ–∑–æ–ª—é—Ü–∏–∏ ---
class VectorClock:
    def __init__(self, node_id: str):
        self.node_id = node_id
        self.versions: Dict[str, int] = {}

    def increment(self):
        self.versions[self.node_id] = self.versions.get(self.node_id, 0) + 1

    def merge(self, other: 'VectorClock') -> bool:
        """
        –û–±'—î–¥–Ω—É—î –≤–µ–∫—Ç–æ—Ä–Ω—ñ –≥–æ–¥–∏–Ω–Ω–∏–∫–∏.
        –ü–æ–≤–µ—Ä—Ç–∞—î True, —è–∫—â–æ –≤–∏—è–≤–ª–µ–Ω–æ –∫–æ–Ω—Ñ–ª—ñ–∫—Ç (–ø–∞—Ä–∞–ª–µ–ª—å–Ω—ñ –∑–º—ñ–Ω–∏).
        """
        has_conflict = False

        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –∫–æ–Ω—Ñ–ª—ñ–∫—Ç:
        # –ö–æ–Ω—Ñ–ª—ñ–∫—Ç —î, —è–∫—â–æ self –º–∞—î —â–æ—Å—å –Ω–æ–≤—ñ—à–µ –∑–∞ other, –ê –¢–ê–ö–û–ñ other –º–∞—î —â–æ—Å—å –Ω–æ–≤—ñ—à–µ –∑–∞ self.
        self_is_newer = False
        other_is_newer = False

        all_nodes = set(self.versions.keys()) | set(other.versions.keys())

        for node in all_nodes:
            v_self = self.versions.get(node, 0)
            v_other = other.versions.get(node, 0)

            if v_self > v_other:
                self_is_newer = True
            elif v_other > v_self:
                other_is_newer = True

        if self_is_newer and other_is_newer:
            has_conflict = True

        # –§–∞–∫—Ç–∏—á–Ω–µ –∑–ª–∏—Ç—Ç—è (–±–µ—Ä–µ–º–æ –º–∞–∫—Å–∏–º—É–º –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –≤—É–∑–ª–∞)
        for node, counter in other.versions.items():
            self.versions[node] = max(self.versions.get(node, 0), counter)

        return has_conflict

    def to_dict(self):
        return self.versions.copy()

    @classmethod
    def from_dict(cls, node_id: str, data: dict):
        vc = cls(node_id)
        vc.versions = data.copy()
        return vc


# --- –í–ù–£–¢–†–Ü–®–ù–Ø –õ–û–ì–Ü–ö–ê –†–ï–ü–õ–Ü–ö–ê–¶–Ü–á ---

def get_lag_seconds() -> float:
    """–í–∏—Ä–∞—Ö–æ–≤—É—î –∑–∞—Ç—Ä–∏–º–∫—É —Ä–µ–ø–ª—ñ–∫–∞—Ü—ñ—ó —É —Å–µ–∫—É–Ω–¥–∞—Ö."""
    if not EVENT_LOG:
        return 0.0

    last_event_ts_str = EVENT_LOG[-1].get("timestamp")
    if last_event_ts_str:
        try:
            last_event_time = datetime.datetime.fromisoformat(last_event_ts_str.replace('Z', '+00:00'))
            current_time = datetime.datetime.now(datetime.timezone.utc)
            lag = (current_time - last_event_time).total_seconds()
            return max(0.0, lag)
        except ValueError:
            return 0.0
    return 0.0


def send_lag_metric(lag_s: float):
    """–£–Ω—ñ—Ñ—ñ–∫–æ–≤–∞–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –≤—ñ–¥–ø—Ä–∞–≤–∫–∏ –º–µ—Ç—Ä–∏–∫–∏ Lag."""
    statlogger.gauge('shard.replication.lag_seconds', lag_s, tags=[f'shard:{SHARD_NAME}', f'group:{SHARD_GROUP}'])


def apply_event_with_conflict_resolution(event: Dict[str, Any]):
    """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —Å–æ–±—ã—Ç–∏–µ —Å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ —á–µ—Ä–µ–∑ Vector Clocks"""
    op = event.get("op")
    table = event.get("table")
    key = event.get("key")
    value = event.get("value")
    event_timestamp = event.get("timestamp")
    event_node = event.get("node_id", SHARD_NAME)

    start_time = time.time()

    try:
        # –°—Ç–≤–æ—Ä—é—î–º–æ –≤–µ–∫—Ç–æ—Ä–Ω—ñ —á–∞—Å–∏ –¥–ª—è –ø–æ–¥—ñ—ó
        event_vc = VectorClock(event_node)
        if "vector_clock" in event:
            event_vc = VectorClock.from_dict(event_node, event["vector_clock"])
        # –¢—É—Ç –º–∏ –Ω–µ —ñ–Ω–∫—Ä–µ–º–µ–Ω—Ç—É—î–º–æ –ª–æ–∫–∞–ª—å–Ω–æ –ø—Ä–∏ apply, –±–æ —Ü–µ –º–æ–∂–µ –±—É—Ç–∏ —Ä–µ–ø–ª—ñ–∫–∞—Ü—ñ—è.
        # –í–µ–∫—Ç–æ—Ä –≤–∂–µ –ø—Ä–∏–π—à–æ–≤ –≥–æ—Ç–æ–≤–∏–π. –ê–ª–µ —è–∫—â–æ —Ü–µ –ª–æ–∫–∞–ª—å–Ω–∏–π create/update, –≤–µ–∫—Ç–æ—Ä —Ñ–æ—Ä–º—É—î—Ç—å—Å—è —Ç–∞–º.

        STORAGE.setdefault(table, {})

        if key in STORAGE[table]:
            current_data = STORAGE[table][key]
            current_vc = VectorClock.from_dict(SHARD_NAME, current_data.get("vector_clock", {}))

            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∫–æ–Ω—Ñ–ª—ñ–∫—Ç
            has_conflict = current_vc.merge(event_vc)

            if has_conflict:
                # --- 2.c CUSTOM MERGE STRATEGY ---
                current_val = current_data.get("value")
                new_val = value

                # –Ø–∫—â–æ –æ–±–∏–¥–≤–∞ –∑–Ω–∞—á–µ–Ω–Ω—è - —Å–ø–∏—Å–∫–∏, –æ–±'—î–¥–Ω—É—î–º–æ —ó—Ö
                if isinstance(current_val, list) and isinstance(new_val, list):
                    # –û–±'—î–¥–Ω—É—î–º–æ –¥–≤–∞ —Å–ø–∏—Å–∫–∏ —ñ –ø—Ä–∏–±–∏—Ä–∞—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç–∏
                    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ set –¥–ª—è —É–Ω—ñ–∫–∞–ª—å–Ω–æ—Å—Ç—ñ, –ø–æ—Ç—ñ–º list
                    try:
                        merged_value = list(set(current_val + new_val))
                    except TypeError:
                        # –Ø–∫—â–æ –µ–ª–µ–º–µ–Ω—Ç–∏ –Ω–µ —Ö–µ—à—É—é—Ç—å—Å—è (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥ dict), –ø—Ä–æ—Å—Ç–æ –¥–æ–¥–∞—î–º–æ
                        merged_value = current_val + new_val

                    # –ë–µ—Ä–µ–º–æ –Ω–∞–π–Ω–æ–≤—ñ—à–∏–π —á–∞—Å
                    new_timestamp = max(event_timestamp, current_data.get("timestamp", ""))

                    STORAGE[table][key] = {
                        "value": merged_value,
                        "version": new_timestamp,
                        "vector_clock": current_vc.to_dict(),  # –í–∂–µ –æ–±'—î–¥–Ω–∞–Ω–∏–π –≤–µ–∫—Ç–æ—Ä
                        "timestamp": new_timestamp
                    }

                    duration_ms = (time.time() - start_time) * 1000
                    statlogger.increment('shard.conflicts.resolved',
                                         tags=[f'shard:{SHARD_NAME}', 'strategy:CustomMerge', f'group:{SHARD_GROUP}'])

                    shard_structured_log("conflict_resolution", key, "resolved",
                                         strategy="Custom_List_Merge",
                                         duration_ms=duration_ms, event_op=op)

                    # –í–ê–ñ–õ–ò–í–û: –î–æ–¥–∞—î–º–æ –ø–æ–¥—ñ—é –≤ –ª–æ–≥ –ø–µ—Ä–µ–¥ –≤–∏—Ö–æ–¥–æ–º
                    EVENT_LOG.append(event)
                    return

                    # --- 2.b Fallback to LWW ---
                current_timestamp = current_data.get("timestamp", "")
                if event_timestamp > current_timestamp:
                    # –ù–æ–≤–∞ –≤–µ—Ä—Å—ñ—è –ø–µ—Ä–µ–º–∞–≥–∞—î
                    STORAGE[table][key] = {
                        "value": value,
                        "version": event_timestamp,
                        "vector_clock": event_vc.to_dict(),  # –ê–±–æ current_vc? –ü—Ä–∏ LWW –∑–∞–∑–≤–∏—á–∞–π –±–µ—Ä–µ–º–æ –ø–µ—Ä–µ–º–æ–∂—Ü—è
                        "timestamp": event_timestamp
                    }
                    statlogger.increment('shard.conflicts.resolved',
                                         tags=[f'shard:{SHARD_NAME}', 'strategy:LWW', f'group:{SHARD_GROUP}'])
                    shard_structured_log("conflict_resolution", key, "resolved",
                                         strategy="LWW", duration_ms=(time.time() - start_time) * 1000)
            else:
                # –ù–µ–º–∞—î –∫–æ–Ω—Ñ–ª—ñ–∫—Ç—É, –ø—Ä–æ—Å—Ç–æ –æ–Ω–æ–≤–ª—é—î–º–æ (Fast Forward)
                # –ê–±–æ —Ü–µ —Å—Ç–∞—Ä–∏–π –∞–ø–¥–µ–π—Ç, —è–∫–∏–π –≤–∂–µ –≤—Ä–∞—Ö–æ–≤–∞–Ω–∏–π —É –≤–µ–∫—Ç–æ—Ä—ñ (—Ç–æ–¥—ñ –Ω—ñ—á–æ–≥–æ –Ω–µ —Ä–æ–±–∏–º–æ? –ù—ñ, LWW).
                # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç–∏ - –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—É—î–º–æ, —è–∫—â–æ —Ü–µ –Ω–µ —Å—Ç–∞—Ä—ñ –¥–∞–Ω—ñ –∑–∞ —á–∞—Å–æ–º.
                current_timestamp = current_data.get("timestamp", "")
                if event_timestamp >= current_timestamp:
                    STORAGE[table][key] = {
                        "value": value,
                        "version": event_timestamp,
                        "vector_clock": event_vc.to_dict(),  # –¢—É—Ç –∫—Ä–∞—â–µ –±—Ä–∞—Ç–∏ current_vc, –±–æ –≤—ñ–Ω –≤–∂–µ merged
                        "timestamp": event_timestamp
                    }
        else:
            # –ù–æ–≤–∏–π –∑–∞–ø–∏—Å
            STORAGE[table][key] = {
                "value": value,
                "version": event_timestamp,
                "vector_clock": event_vc.to_dict(),
                "timestamp": event_timestamp
            }

        EVENT_LOG.append(event)
        duration_ms = (time.time() - start_time) * 1000
        shard_structured_log("apply_event", key, "success", duration_ms=duration_ms, event_op=op)

    except Exception as e:
        duration_ms = (time.time() - start_time) * 1000
        shard_structured_log("apply_event", key, "error", str(e), duration_ms, event_op=op)
        raise


async def sync_with_leader():
    """
    (–¢—ñ–ª—å–∫–∏ –¥–ª—è —Ñ–æ–ª–æ–≤–µ—Ä—ñ–≤)
    –ó–≤–µ—Ä—Ç–∞—î—Ç—å—Å—è –¥–æ –ª—ñ–¥–µ—Ä–∞, —â–æ–± –æ—Ç—Ä–∏–º–∞—Ç–∏ –ø—Ä–æ–ø—É—â–µ–Ω—ñ –ø–æ–¥—ñ—ó.
    """
    if REPLICA_ROLE != "follower" or not LEADER_URL:
        return

    current_offset = len(EVENT_LOG)
    start_time = time.time()

    try:
        async with httpx.AsyncClient() as client:
            resp = await client.get(f"{LEADER_URL}/sync", params={"offset": current_offset}, timeout=30.0)

            if resp.status_code != 200:
                shard_structured_log("sync", status="error", error=f"HTTP {resp.status_code}",
                                     duration_ms=(time.time() - start_time) * 1000)
                statlogger.increment('shard.sync.errors', tags=[f'shard:{SHARD_NAME}', f'group:{SHARD_GROUP}'])
                return

            missing_events = resp.json().get("events", [])
            events_count = len(missing_events)

            statlogger.gauge('shard.replication.lag_size', events_count,
                             tags=[f'shard:{SHARD_NAME}', f'group:{SHARD_GROUP}'])

            if missing_events:
                for event in missing_events:
                    apply_event_with_conflict_resolution(event)

                duration_ms = (time.time() - start_time) * 1000
                shard_structured_log("sync", status="success", duration_ms=duration_ms,
                                     events_synced=events_count)
                statlogger.increment('shard.sync.events_received', events_count,
                                     tags=[f'shard:{SHARD_NAME}', f'group:{SHARD_GROUP}'])

    except Exception as e:
        duration_ms = (time.time() - start_time) * 1000
        shard_structured_log("sync", status="error", error=str(e), duration_ms=duration_ms)
        statlogger.increment('shard.sync.errors', tags=[f'shard:{SHARD_NAME}', f'group:{SHARD_GROUP}'])


# --- Prometheus –º–µ—Ç—Ä–∏–∫–∏ ---
@app.get("/metrics")
def prometheus_metrics():
    """Prometheus-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —à–∞—Ä–¥–∞"""
    metrics = []

    # Throughput
    metrics.append(f"shard_throughput_writes_total{{shard=\"{SHARD_NAME}\"}} {SHARD_COUNTERS['write_success']}")
    metrics.append(f"shard_throughput_reads_total{{shard=\"{SHARD_NAME}\"}} {SHARD_COUNTERS['read_success']}")

    # Error rate
    total_writes = SHARD_COUNTERS['write_success'] + SHARD_COUNTERS['write_error']
    total_reads = SHARD_COUNTERS['read_success'] + SHARD_COUNTERS['read_error']

    write_error_rate = SHARD_COUNTERS['write_error'] / total_writes if total_writes > 0 else 0
    read_error_rate = SHARD_COUNTERS['read_error'] / total_reads if total_reads > 0 else 0

    metrics.append(f"shard_error_rate_write{{shard=\"{SHARD_NAME}\"}} {write_error_rate}")
    metrics.append(f"shard_error_rate_read{{shard=\"{SHARD_NAME}\"}} {read_error_rate}")

    # Storage metrics
    total_keys = sum(len(table) for table in STORAGE.values())
    metrics.append(f"shard_storage_keys_total{{shard=\"{SHARD_NAME}\"}} {total_keys}")
    metrics.append(f"shard_event_log_size{{shard=\"{SHARD_NAME}\"}} {len(EVENT_LOG)}")

    # Replication metrics
    metrics.append(f"shard_replication_received_total{{shard=\"{SHARD_NAME}\"}} {SHARD_COUNTERS['replication_received']}")
    metrics.append(f"shard_sync_requests_total{{shard=\"{SHARD_NAME}\"}} {SHARD_COUNTERS['sync_requests']}")

    return Response("\n".join(metrics), mimetype="text/plain")


# --- Health check ---
@app.get("/health")
def health_check():
    """–ï–Ω–¥–ø–æ—ñ–Ω—Ç –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏ –∑–¥–æ—Ä–æ–≤'—è –≤—É–∑–ª–∞."""
    health_data = {
        "status": "ok",
        "role": REPLICA_ROLE,
        "shard": SHARD_NAME,
        "group": SHARD_GROUP,
        "log_size": len(EVENT_LOG),
        "storage_keys": sum(len(table) for table in STORAGE.values()),
        "lag_seconds": get_lag_seconds() if REPLICA_ROLE == "follower" else 0,
        "timestamp": datetime.datetime.utcnow().isoformat()
    }

    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫—É –∑–¥–æ—Ä–æ–≤—å—è
    health_value = 1
    statlogger.gauge('shard.health.status', health_value,
                     tags=[f'shard:{SHARD_NAME}', f'group:{SHARD_GROUP}', f'role:{REPLICA_ROLE}'])

    return health_data

# --- VERSION ENDPOINT
@app.get("/version")
def get_version():
    """–ü–æ–≤–µ—Ä—Ç–∞—î –≤–µ—Ä—Å—ñ—é –±—ñ–ª–¥–∞"""
    return {
        "version": "1.0.0",
        "build_date": datetime.datetime.utcnow().isoformat(),
        "k8s_node": SHARD_NAME
    }

async def self_health_check() -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Ç–µ–∫—É—â–µ–≥–æ —à–∞—Ä–¥–∞"""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–æ–∂–µ–º –ø–∏—Å–∞—Ç—å/—á–∏—Ç–∞—Ç—å –∏–∑ —Å–≤–æ–µ–≥–æ –∂–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        test_key = f"health_check_{SHARD_NAME}_{int(time.time())}"
        STORAGE.setdefault("__health", {})
        STORAGE["__health"][test_key] = {
            "value": "test",
            "version": datetime.datetime.utcnow().isoformat(),
            "vector_clock": {SHARD_NAME: 1},
            "timestamp": datetime.datetime.utcnow().isoformat()
        }
        # –ß–∏—Å—Ç–∏–º–æ –∑–∞ —Å–æ–±–æ—é
        del STORAGE["__health"][test_key]
        return True
    except Exception as e:
        shard_structured_log("health_check", status="error", error=str(e))
        return False


# --- –ó–ê–ü–£–°–ö –¢–ê –†–ï–Ñ–°–¢–†–ê–¶–Ü–Ø ---
@app.on_event("startup")
async def register_and_sync():
    """
    1. –†–µ—î—Å—Ç—Ä—É—î shard –ø—Ä–∏ –∑–∞–ø—É—Å–∫—É (–≤–∫–ª—é—á–Ω–æ –∑ —Ä–æ–ª–ª—é —ñ –≥—Ä—É–ø–æ—é).
    2. –ó–∞–ø—É—Å–∫–∞—î —î–¥–∏–Ω–∏–π —Ü–∏–∫–ª –º–µ—Ç—Ä–∏–∫ —Ç–∞ —Å–∏–Ω—Ö—Ä–æ–Ω—ñ–∑–∞—Ü—ñ—ó.
    """
    payload = {
        "name": SHARD_NAME,
        "url": f"http://{SHARD_NAME}:8000",
        "group": SHARD_GROUP,
        "role": REPLICA_ROLE
    }

    print(f"üß≠ Registering shard with coordinator: {payload}")
    statlogger.event('Shard Startup', f'Shard {SHARD_NAME} starting up with role {REPLICA_ROLE}', alert_type='info')

    # --- –ü–û–ö–†–ê–©–ï–ù–ò–ô –¶–∏–∫–ª –ø–æ–≤—Ç–æ—Ä–Ω–∏—Ö —Å–ø—Ä–æ–± —Ä–µ—î—Å—Ç—Ä–∞—Ü—ñ—ó ---
    registered = False
    for attempt in range(20):  # 20 —Å–ø—Ä–æ–± * 3 —Å–µ–∫ = 60 —Å–µ–∫ –æ—á—ñ–∫—É–≤–∞–Ω–Ω—è
        try:
            async with httpx.AsyncClient() as client:
                resp = await client.post(f"{COORDINATOR_URL}/register_shard", json=payload, timeout=5.0)

                if resp.status_code == 200:
                    shard_structured_log("registration", status="success",
                                         coordinator_response=resp.status_code)
                    print(f"‚úÖ Coordinator responded: {resp.status_code} - {resp.text}")
                    registered = True
                    break
                else:
                    print(f"‚ö†Ô∏è Attempt {attempt + 1}: Registration returned {resp.status_code}. Retrying...")

        except Exception as e:
            print(f"‚ö†Ô∏è Attempt {attempt + 1}: Failed to connect to coordinator ({e}). Retrying in 3s...")

        await asyncio.sleep(3)

    if not registered:
        shard_structured_log("registration", status="fatal", error="Could not register after multiple attempts")
        print("‚ùå FATAL: Could not register shard. Continuing without registration (Metrics only).")

    # –ó–∞–ø—É—Å–∫ —î–¥–∏–Ω–æ–≥–æ —Ü–∏–∫–ª—É –¥–ª—è –º–µ—Ç—Ä–∏–∫ —Ç–∞ —Å–∏–Ω—Ö—Ä–æ–Ω—ñ–∑–∞—Ü—ñ—ó
    asyncio.create_task(periodic_metrics_and_sync())


async def periodic_metrics_and_sync():
    """–û–±'—î–¥–Ω—É—î —Å–∏–Ω—Ö—Ä–æ–Ω—ñ–∑–∞—Ü—ñ—é —Ç–∞ –≤—ñ–¥–ø—Ä–∞–≤–∫—É –º–µ—Ç—Ä–∏–∫ —É —Ñ–æ–Ω–æ–≤–æ–º—É —Ä–µ–∂–∏–º—ñ."""
    while True:
        await asyncio.sleep(10)  # –ö–æ–∂–Ω—ñ 10 —Å–µ–∫—É–Ω–¥

        # 1. –°–ò–ù–•–†–û–ù–Ü–ó–ê–¶–Ü–Ø (–¢—ñ–ª—å–∫–∏ –¥–ª—è —Ñ–æ–ª–æ–≤–µ—Ä—ñ–≤)
        if REPLICA_ROLE == "follower":
            await sync_with_leader()

        # 2. –í–Ü–î–ü–†–ê–í–ö–ê –ú–ï–¢–†–ò–ö–ò LAG (–î–ª—è –≤—Å—ñ—Ö)
        send_lag_metric(get_lag_seconds())

        # 3. –ú–µ—Ç—Ä–∏–∫–∏ –∑–¥–æ—Ä–æ–≤—å—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        total_keys = sum(len(table) for table in STORAGE.values())
        statlogger.gauge('shard.storage.keys_total', total_keys,
                         tags=[f'shard:{SHARD_NAME}', f'group:{SHARD_GROUP}'])

        # 4. –ú–µ—Ç—Ä–∏–∫–∏ —Ä–∞–∑–º–µ—Ä–∞ –ª–æ–≥–∞
        statlogger.gauge('shard.event_log.size', len(EVENT_LOG),
                         tags=[f'shard:{SHARD_NAME}', f'group:{SHARD_GROUP}'])

        # 5. Health check –º–µ—Ç—Ä–∏–∫–∞
        health_status = 1 if await self_health_check() else 0
        statlogger.gauge('shard.health.status', health_status,
                         tags=[f'shard:{SHARD_NAME}', f'group:{SHARD_GROUP}', f'role:{REPLICA_ROLE}'])


def _composite(k: str, sk: Optional[str]) -> str:
    return f"{k}:{sk}" if sk else k


async def fanout_to_followers(event: dict):
    start_time = time.time()
    async with httpx.AsyncClient() as client:
        valid_urls = [url for url in FOLLOWER_URLS if url]

        tasks = []
        for url in valid_urls:
            tasks.append(client.post(f"{url}/replicate", json=event, timeout=5))

        responses = await asyncio.gather(*tasks, return_exceptions=True)

        replication_time_ms = (time.time() - start_time) * 1000
        statlogger.histogram('shard.replication.fanout_latency', replication_time_ms,
                             tags=[f'shard:{SHARD_NAME}', f'group:{SHARD_GROUP}'])

        successful_replications = 0
        for url, resp in zip(valid_urls, responses):
            if isinstance(resp, Exception) or (isinstance(resp, httpx.Response) and resp.status_code != 200):
                shard_structured_log("fanout", status="error", error=str(resp),
                                     duration_ms=replication_time_ms, target=url)
                statlogger.increment('shard.replication.fanout_errors',
                                     tags=[f'shard:{SHARD_NAME}', f'group:{SHARD_GROUP}'])
            else:
                successful_replications += 1

        statlogger.gauge('shard.replication.fanout_success', successful_replications,
                         tags=[f'shard:{SHARD_NAME}', f'group:{SHARD_GROUP}'])


# --- CRUD (–ó–ê–ü–ò–°) ---
@app.post("/create")
@tracer.wrap(service='shard-service', resource='create')
async def create(data: KeyValue):
    start_time = time.time()
    key = _composite(data.key, data.sort_key)

    if REPLICA_ROLE != "leader":
        duration_ms = (time.time() - start_time) * 1000
        shard_structured_log("create", key, "error", "Forbidden: This node is a follower", duration_ms)
        SHARD_COUNTERS["write_error"] += 1
        statlogger.increment('shard.errors.4xx', tags=[f'shard:{SHARD_NAME}', 'op:create'])
        raise HTTPException(403, f"Forbidden: This node is a follower.")

    try:
        event = {
            "op": "create",
            "table": data.table,
            "key": key,
            "value": data.value,
            "timestamp": datetime.datetime.utcnow().isoformat(),
            "node_id": SHARD_NAME,
            "vector_clock": {SHARD_NAME: 1}
        }
        apply_event_with_conflict_resolution(event)

        asyncio.create_task(fanout_to_followers(event))

        duration_ms = (time.time() - start_time) * 1000
        SHARD_COUNTERS["write_success"] += 1
        statlogger.histogram('shard.latency.write', duration_ms, tags=[f'shard:{SHARD_NAME}', 'op:create'])
        statlogger.increment('shard.throughput.writes',
                             tags=[f'shard:{SHARD_NAME}', 'op:create', f'group:{SHARD_GROUP}'])
        shard_structured_log("create", key, "success", duration_ms=duration_ms)

        return {"message": "created", "key": key, "version": event["timestamp"]}
    except Exception as e:
        duration_ms = (time.time() - start_time) * 1000
        SHARD_COUNTERS["write_error"] += 1
        shard_structured_log("create", key, "error", str(e), duration_ms)
        statlogger.increment('shard.errors.5xx', tags=[f'shard:{SHARD_NAME}', 'op:create'])
        raise HTTPException(500, f"Internal error during create: {e}")


@app.put("/update")
@tracer.wrap(service='shard-service', resource='update')
async def update(data: KeyValue):
    start_time = time.time()
    key = _composite(data.key, data.sort_key)

    if REPLICA_ROLE != "leader":
        duration_ms = (time.time() - start_time) * 1000
        shard_structured_log("update", key, "error", "Forbidden: This node is a follower", duration_ms)
        SHARD_COUNTERS["write_error"] += 1
        statlogger.increment('shard.errors.4xx', tags=[f'shard:{SHARD_NAME}', 'op:update'])
        raise HTTPException(403, f"Forbidden: This node is a follower.")

    if data.table not in STORAGE or key not in STORAGE[data.table]:
        duration_ms = (time.time() - start_time) * 1000
        shard_structured_log("update", key, "error", "Key not found", duration_ms)
        SHARD_COUNTERS["write_error"] += 1
        statlogger.increment('shard.errors.4xx', tags=[f'shard:{SHARD_NAME}', 'op:update'])
        raise HTTPException(404, "Key not found")

    try:
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ —á–∞—Å—ã
        current_data = STORAGE[data.table][key]
        current_vc = VectorClock.from_dict(SHARD_NAME, current_data.get("vector_clock", {}))
        current_vc.increment()

        event = {
            "op": "update",
            "table": data.table,
            "key": key,
            "value": data.value,
            "timestamp": datetime.datetime.utcnow().isoformat(),
            "node_id": SHARD_NAME,
            "vector_clock": current_vc.to_dict()
        }
        apply_event_with_conflict_resolution(event)

        asyncio.create_task(fanout_to_followers(event))

        duration_ms = (time.time() - start_time) * 1000
        SHARD_COUNTERS["write_success"] += 1
        statlogger.histogram('shard.latency.write', duration_ms, tags=[f'shard:{SHARD_NAME}', 'op:update'])
        statlogger.increment('shard.throughput.writes',
                             tags=[f'shard:{SHARD_NAME}', 'op:update', f'group:{SHARD_GROUP}'])
        shard_structured_log("update", key, "success", duration_ms=duration_ms)

        return {"message": "updated", "key": key, "version": event["timestamp"]}
    except Exception as e:
        duration_ms = (time.time() - start_time) * 1000
        SHARD_COUNTERS["write_error"] += 1
        shard_structured_log("update", key, "error", str(e), duration_ms)
        statlogger.increment('shard.errors.5xx', tags=[f'shard:{SHARD_NAME}', 'op:update'])
        raise HTTPException(500, f"Internal error during update: {e}")


@app.delete("/delete/{table}/{key}")
@tracer.wrap(service='shard-service', resource='delete')
async def delete(table: str, key: str, sort_key: Optional[str] = Query(None)):
    start_time = time.time()
    comp = _composite(key, sort_key)

    if REPLICA_ROLE != "leader":
        duration_ms = (time.time() - start_time) * 1000
        shard_structured_log("delete", comp, "error", "Forbidden: This node is a follower", duration_ms)
        SHARD_COUNTERS["write_error"] += 1
        statlogger.increment('shard.errors.4xx', tags=[f'shard:{SHARD_NAME}', 'op:delete'])
        raise HTTPException(403, f"Forbidden: This node is a follower.")

    if table not in STORAGE or comp not in STORAGE[table]:
        duration_ms = (time.time() - start_time) * 1000
        shard_structured_log("delete", comp, "error", "Key not found", duration_ms)
        SHARD_COUNTERS["write_error"] += 1
        statlogger.increment('shard.errors.4xx', tags=[f'shard:{SHARD_NAME}', 'op:delete'])
        raise HTTPException(404, "not found")

    try:
        event = {
            "op": "delete",
            "table": table,
            "key": comp,
            "timestamp": datetime.datetime.utcnow().isoformat(),
            "node_id": SHARD_NAME
        }
        apply_event_with_conflict_resolution(event)

        asyncio.create_task(fanout_to_followers(event))

        duration_ms = (time.time() - start_time) * 1000
        SHARD_COUNTERS["write_success"] += 1
        statlogger.histogram('shard.latency.write', duration_ms, tags=[f'shard:{SHARD_NAME}', 'op:delete'])
        statlogger.increment('shard.throughput.writes',
                             tags=[f'shard:{SHARD_NAME}', 'op:delete', f'group:{SHARD_GROUP}'])
        shard_structured_log("delete", comp, "success", duration_ms=duration_ms)

        return {"message": "deleted", "key": comp}
    except Exception as e:
        duration_ms = (time.time() - start_time) * 1000
        SHARD_COUNTERS["write_error"] += 1
        shard_structured_log("delete", comp, "error", str(e), duration_ms)
        statlogger.increment('shard.errors.5xx', tags=[f'shard:{SHARD_NAME}', 'op:delete'])
        raise HTTPException(500, f"Internal error during delete: {e}")


# --- CRUD (–ß–ò–¢–ê–ù–ù–Ø) ---
@app.get("/read/{table}/{key}")
@tracer.wrap(service='shard-service', resource='read')
def read(table: str, key: str, sort_key: Optional[str] = Query(None)):
    start_time = time.time()
    comp = _composite(key, sort_key)

    try:
        if table not in STORAGE or comp not in STORAGE[table]:
            duration_ms = (time.time() - start_time) * 1000
            SHARD_COUNTERS["read_error"] += 1
            shard_structured_log("read", comp, "error", "Key not found", duration_ms)
            statlogger.increment('shard.errors.4xx', tags=[f'shard:{SHARD_NAME}', 'op:read'])
            raise HTTPException(404, "not found")

        data = STORAGE[table][comp]
        duration_ms = (time.time() - start_time) * 1000

        SHARD_COUNTERS["read_success"] += 1
        statlogger.histogram('shard.latency.read', duration_ms, tags=[f'shard:{SHARD_NAME}', 'op:read'])
        statlogger.increment('shard.throughput.reads', tags=[f'shard:{SHARD_NAME}', 'op:read', f'group:{SHARD_GROUP}'])
        shard_structured_log("read", comp, "success", duration_ms=duration_ms)

        # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –¥–∞–Ω—ñ —Ä–∞–∑–æ–º –∑ –º–µ—Ç–∞–¥–∞–Ω–∏–º–∏
        return {"key": comp, "data": data, "served_by": SHARD_NAME}
    except Exception as e:
        duration_ms = (time.time() - start_time) * 1000
        SHARD_COUNTERS["read_error"] += 1
        shard_structured_log("read", comp, "error", str(e), duration_ms)
        statlogger.increment('shard.errors.5xx', tags=[f'shard:{SHARD_NAME}', 'op:read'])
        raise HTTPException(500, f"Internal error during read: {e}")


@app.get("/exists/{table}/{key}")
@tracer.wrap(service='shard-service', resource='exists')
def exists(table: str, key: str, sort_key: Optional[str] = Query(None)):
    start_time = time.time()
    comp = _composite(key, sort_key)

    try:
        exists = comp in STORAGE.get(table, {})
        duration_ms = (time.time() - start_time) * 1000

        if exists:
            SHARD_COUNTERS["read_success"] += 1
        else:
            SHARD_COUNTERS["read_error"] += 1
            statlogger.increment('shard.errors.4xx', tags=[f'shard:{SHARD_NAME}', 'op:exists'])

        statlogger.histogram('shard.latency.read', duration_ms, tags=[f'shard:{SHARD_NAME}', 'op:exists'])
        statlogger.increment('shard.throughput.reads',
                             tags=[f'shard:{SHARD_NAME}', 'op:exists', f'group:{SHARD_GROUP}'])
        shard_structured_log("exists", comp, "success", duration_ms=duration_ms, exists=exists)

        return {"exists": exists, "served_by": SHARD_NAME}
    except Exception as e:
        duration_ms = (time.time() - start_time) * 1000
        SHARD_COUNTERS["read_error"] += 1
        shard_structured_log("exists", comp, "error", str(e), duration_ms)
        statlogger.increment('shard.errors.5xx', tags=[f'shard:{SHARD_NAME}', 'op:exists'])
        raise HTTPException(500, f"Internal error during exists: {e}")


# --- –ï–ù–î–ü–û–Ü–ù–¢–ò –†–ï–ü–õ–Ü–ö–ê–¶–Ü–á ---
@app.post("/replicate")
@tracer.wrap(service='shard-service', resource='replicate')
def replicate(event: Dict[str, Any]):
    start_time = time.time()

    if REPLICA_ROLE == "leader":
        duration_ms = (time.time() - start_time) * 1000
        shard_structured_log("replicate", status="error", error="Leader cannot replicate from itself",
                             duration_ms=duration_ms)
        raise HTTPException(500, "Leader cannot replicate from itself")

    try:
        apply_event_with_conflict_resolution(event)

        duration_ms = (time.time() - start_time) * 1000
        SHARD_COUNTERS["replication_received"] += 1
        statlogger.increment('shard.replication.received', tags=[f'shard:{SHARD_NAME}'])
        # –í–ò–ü–†–ê–í–õ–ï–ù–û: event_op –∑–∞–º—ñ—Å—Ç—å operation
        shard_structured_log("replicate", event.get("key"), "success", duration_ms=duration_ms,
                             event_op=event.get("op"))

        return {"replicated": True, "op": event.get("op"), "node": SHARD_NAME}
    except Exception as e:
        duration_ms = (time.time() - start_time) * 1000
        SHARD_COUNTERS["replication_errors"] += 1
        # –í–ò–ü–†–ê–í–õ–ï–ù–û: event_op –∑–∞–º—ñ—Å—Ç—å operation
        shard_structured_log("replicate", event.get("key"), "error", str(e), duration_ms, event_op=event.get("op"))
        statlogger.increment('shard.replication.errors', tags=[f'shard:{SHARD_NAME}'])
        raise HTTPException(500, f"Replication failed: {e}")


@app.get("/sync")
@tracer.wrap(service='shard-service', resource='sync')
def sync(offset: int = 0):
    start_time = time.time()

    if REPLICA_ROLE != "leader":
        duration_ms = (time.time() - start_time) * 1000
        shard_structured_log("sync", status="error", error="Forbidden: This node is a follower",
                             duration_ms=duration_ms)
        SHARD_COUNTERS["sync_requests"] += 1
        statlogger.increment('shard.errors.4xx', tags=[f'shard:{SHARD_NAME}', 'op:sync'])
        raise HTTPException(403, f"Forbidden: This node is a follower.")

    try:
        if offset < 0:
            offset = 0

        missing_events = EVENT_LOG[offset:]
        duration_ms = (time.time() - start_time) * 1000

        SHARD_COUNTERS["sync_requests"] += 1
        statlogger.increment('shard.sync.requests', tags=[f'shard:{SHARD_NAME}'])
        statlogger.gauge('shard.sync.events_sent', len(missing_events), tags=[f'shard:{SHARD_NAME}'])
        shard_structured_log("sync", status="success", duration_ms=duration_ms, events_sent=len(missing_events))

        return {"events": missing_events, "current_leader_offset": len(EVENT_LOG)}
    except Exception as e:
        duration_ms = (time.time() - start_time) * 1000
        SHARD_COUNTERS["sync_requests"] += 1
        shard_structured_log("sync", status="error", error=str(e), duration_ms=duration_ms)
        statlogger.increment('shard.sync.errors', tags=[f'shard:{SHARD_NAME}'])
        raise HTTPException(500, f"Sync failed: {e}")


# === –õ–û–ì–Ü–ö–ê –†–ï–ë–ê–õ–ê–ù–°–£–í–ê–ù–ù–Ø (LAB 5) ===

class MigrationBatch(BaseModel):
    shard_source: str
    data: List[KeyValue]


@app.post("/migration/receive")
async def receive_migration_batch(batch: MigrationBatch):
    """
    –ü—Ä–∏–π–º–∞—î –ø–∞–∫–µ—Ç –¥–∞–Ω–∏—Ö –≤—ñ–¥ —ñ–Ω—à–æ–≥–æ —à–∞—Ä–¥—É, —è–∫–∏–π –≤–∏—Ä—ñ—à–∏–≤,
    —â–æ —Ü—ñ –∫–ª—é—á—ñ —Ç–µ–ø–µ—Ä –Ω–∞–ª–µ–∂–∞—Ç—å –Ω–∞–º.
    """
    count = 0
    start_time = time.time()
    try:
        # –ó–∞–ø–∏—Å—É—î–º–æ –¥–∞–Ω—ñ "—Ç–∏—Ö–æ", –±–µ–∑ —Ä–µ–ø–ª—ñ–∫–∞—Ü—ñ—ó (–±–æ –º–∏ —Ç–µ–ø–µ—Ä –≤–ª–∞—Å–Ω–∏–∫–∏)
        for item in batch.data:
            key_composite = _composite(item.key, item.sort_key)

            # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è —Ç–∞–∫–∞ —Å–∞–º–∞, —è–∫ —É apply_event
            STORAGE.setdefault(item.table, {})
            STORAGE[item.table][key_composite] = {
                "value": item.value,
                "version": datetime.datetime.utcnow().isoformat(),  # –ê–±–æ –∑–±–µ—Ä–µ–≥—Ç–∏ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π timestamp
                "vector_clock": {SHARD_NAME: 1},  # –°–∫–∏–¥–∞—î–º–æ –≤–µ–∫—Ç–æ—Ä –∞–±–æ –º–µ—Ä–¥–∂–∏–º–æ
                "timestamp": datetime.datetime.utcnow().isoformat()
            }
            count += 1

        duration = (time.time() - start_time) * 1000
        shard_structured_log("migration_receive", status="success", count=count, duration_ms=duration)
        return {"status": "ok", "received": count}
    except Exception as e:
        shard_structured_log("migration_receive", status="error", error=str(e))
        raise HTTPException(500, str(e))


class RebalanceRequest(BaseModel):
    active_shards: List[str]  # –°–ø–∏—Å–æ–∫ —ñ–º–µ–Ω –≤—Å—ñ—Ö –∂–∏–≤–∏—Ö —à–∞—Ä–¥—ñ–≤: ["shard-node-0", "shard-node-1"]


@app.post("/rebalance")
async def start_rebalance(req: RebalanceRequest):
    """
    –ö–æ–º–∞–Ω–¥–∞ –≤—ñ–¥ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–∞: "–¢–æ–ø–æ–ª–æ–≥—ñ—è –∑–º—ñ–Ω–∏–ª–∞—Å—å, –ø–µ—Ä–µ–≤—ñ—Ä —Å–≤–æ—ó –∫–ª—é—á—ñ!"
    """
    # –ó–∞–ø—É—Å–∫–∞—î–º–æ —É —Ñ–æ–Ω—ñ, —â–æ–± –Ω–µ –±–ª–æ–∫—É–≤–∞—Ç–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–∞
    asyncio.create_task(process_rebalancing(req.active_shards))
    return {"status": "rebalancing_started"}


async def process_rebalancing(active_shards: List[str]):
    """
    –ü—Ä–æ—Ö–æ–¥–∏—Ç—å –ø–æ –≤—Å—ñ—Ö –∫–ª—é—á–∞—Ö —ñ –ø–µ—Ä–µ–≤—ñ—Ä—è—î, —á–∏ –º–∏ –≤—Å–µ —â–µ –≤–ª–∞—Å–Ω–∏–∫–∏.
    """
    shard_logger.info(f"‚öñÔ∏è Starting rebalance. Active ring: {active_shards}")

    # 1. –ë—É–¥—É—î–º–æ –∞–∫—Ç—É–∞–ª—å–Ω–µ –∫—ñ–ª—å—Ü–µ
    ring = ConsistentHashRing()
    for node in active_shards:
        ring.add_node(node)

    keys_to_move: Dict[str, List[KeyValue]] = {}  # target_node -> list of keys
    keys_to_delete: List[tuple] = []  # (table, key)

    # 2. –°–∫–∞–Ω—É—î–º–æ –ª–æ–∫–∞–ª—å–Ω–µ —Å—Ö–æ–≤–∏—â–µ
    # –í–ê–ñ–õ–ò–í–û: –†–æ–±–∏–º–æ –∫–æ–ø—ñ—é –∫–ª—é—á—ñ–≤, –±–æ –±—É–¥–µ–º–æ –≤–∏–¥–∞–ª—è—Ç–∏
    tables_snapshot = list(STORAGE.keys())

    for table in tables_snapshot:
        if table not in STORAGE: continue

        # –ö–æ–ø—ñ—é—î–º–æ –∫–ª—é—á—ñ —Ç–∞–±–ª–∏—Ü—ñ
        keys_snapshot = list(STORAGE[table].keys())

        for key in keys_snapshot:
            # –í–∏–∑–Ω–∞—á–∞—î–º–æ, —Ö—Ç–æ –º–∞—î –±—É—Ç–∏ –≤–ª–∞—Å–Ω–∏–∫–æ–º –∫–ª—é—á–∞ —É –Ω–æ–≤—ñ–π —Ç–æ–ø–æ–ª–æ–≥—ñ—ó
            correct_owner = ring.get_node(key)

            # –Ø–∫—â–æ –≤–ª–∞—Å–Ω–∏–∫ –Ω–µ —è (—ñ –Ω–µ –º–æ—è –≥—Ä—É–ø–∞) ‚Äî —Ç—Ä–µ–±–∞ –ø–µ—Ä–µ—Å–∏–ª–∞—Ç–∏
            # (–ü–æ—Ä—ñ–≤–Ω—é—î–º–æ –∑ SHARD_NAME, —è–∫–∏–π –≤ K8s = "shard-node-X")
            if correct_owner and correct_owner != SHARD_NAME:

                # –§–æ—Ä–º—É—î–º–æ –æ–±'—î–∫—Ç –¥–ª—è –≤—ñ–¥–ø—Ä–∞–≤–∫–∏
                raw_data = STORAGE[table][key]
                kv = KeyValue(
                    table=table,
                    key=key.split(":")[0],
                    sort_key=key.split(":")[1] if ":" in key else None,
                    value=raw_data["value"]
                )

                if correct_owner not in keys_to_move:
                    keys_to_move[correct_owner] = []
                keys_to_move[correct_owner].append(kv)

                keys_to_delete.append((table, key))

    # 3. –í—ñ–¥–ø—Ä–∞–≤–ª—è—î–º–æ –¥–∞–Ω—ñ –Ω–æ–≤–∏–º –≤–ª–∞—Å–Ω–∏–∫–∞–º
    client = httpx.AsyncClient()
    for target_node, items in keys_to_move.items():
        try:
            # –§–æ—Ä–º—É—î–º–æ URL: http://shard-node-X.shard-service:8000/migration/receive
            # –ü—Ä–∏–ø—É—Å–∫–∞—î–º–æ, —â–æ target_node ‚Äî —Ü–µ DNS —ñ–º'—è –ø–æ–¥–∞
            target_url = f"http://{target_node}.{os.getenv('SHARD_SERVICE_NAME', 'shard-service')}:8000/migration/receive"

            shard_logger.info(f"üì¶ Migrating {len(items)} keys to {target_node}...")

            payload = {
                "shard_source": SHARD_NAME,
                "data": [item.dict() for item in items]
            }

            resp = await client.post(target_url, json=payload, timeout=10.0)

            if resp.status_code == 200:
                shard_logger.info(f"‚úÖ Migration to {target_node} successful.")
            else:
                shard_logger.error(f"‚ùå Migration failed: {resp.text}")
                # –£ —Ä–µ–∞–ª—å–Ω–æ–º—É –ø—Ä–æ–¥—ñ –º–∏ –± –Ω–µ –≤–∏–¥–∞–ª—è–ª–∏ –∫–ª—é—á—ñ, —è–∫—â–æ –≤—ñ–¥–ø—Ä–∞–≤–∫–∞ –Ω–µ –≤–¥–∞–ª–∞—Å—å
                # –ê–ª–µ –¥–ª—è –ª–∞–±–∏ –º–æ–∂–Ω–∞ —Å–ø—Ä–æ—Å—Ç–∏—Ç–∏

        except Exception as e:
            shard_logger.error(f"‚ùå Migration error to {target_node}: {e}")

    await client.aclose()

    # 4. –í–∏–¥–∞–ª—è—î–º–æ –ø–µ—Ä–µ–¥–∞–Ω—ñ –∫–ª—é—á—ñ —É —Å–µ–±–µ
    # (–í–∏–∫–æ–Ω—É—î–º–æ –≤–∏–º–æ–≥—É: "Only migrate keys that now belong to different shard")
    for table, key in keys_to_delete:
        if table in STORAGE and key in STORAGE[table]:
            del STORAGE[table][key]

    shard_logger.info(f"‚öñÔ∏è Rebalance complete. Moved {len(keys_to_delete)} keys.")